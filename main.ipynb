{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Import üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, TrainingArguments, Trainer,DataCollatorForTokenClassification,get_scheduler\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Parsing üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllx(filepath):\n",
    "    sentences_tokens = []\n",
    "    sentences_tags = []\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences_tokens.append(tokens)\n",
    "                    sentences_tags.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                tokens.append(parts[1])\n",
    "                tags.append(parts[4])\n",
    "        if tokens:\n",
    "            sentences_tokens.append(tokens)\n",
    "            sentences_tags.append(tags)\n",
    "    return sentences_tokens, sentences_tags\n",
    "\n",
    "def load_data_from_folders(folders, base_path=\"wsj\"):\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        for file in glob.glob(os.path.join(folder_path, \"*.conllx\")):\n",
    "            tokens, tags = read_conllx(file)\n",
    "            all_tokens.extend(tokens)\n",
    "            all_tags.extend(tags)\n",
    "    return all_tokens, all_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split üóÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  {'$', 'CD', 'IN', 'RP', 'CC', 'WP', 'WRB', '#', 'MD', '``', 'VBG', '-LRB-', 'RBS', 'RBR', 'EX', 'VB', 'RB', 'FW', 'JJR', 'NNS', 'SYM', 'NNPS', 'UH', 'PRP$', 'VBN', 'PDT', 'LS', 'POS', 'WP$', 'TO', 'DT', 'VBP', 'JJS', 'WDT', 'NN', \"''\", 'PRP', 'JJ', 'NNP', 'VBD', ':', '.', ',', 'VBZ', '-RRB-'}\n",
      "b  {'$', 'CD', 'IN', 'RP', 'CC', 'WP', 'WRB', '#', 'MD', '``', 'VBG', '-LRB-', 'RBS', 'RBR', 'EX', 'VB', 'RB', 'FW', 'JJR', 'NNS', 'SYM', 'NNPS', 'UH', 'PDT', 'VBN', 'PRP$', 'LS', 'POS', 'WP$', 'TO', 'DT', 'VBP', 'JJS', 'WDT', 'NN', \"''\", 'PRP', 'JJ', 'NNP', 'VBD', ':', '.', ',', 'VBZ', '-RRB-'}\n",
      "c  {'$', 'CD', 'IN', 'RP', 'CC', 'WP', 'WRB', '#', 'MD', '``', 'VBG', '-LRB-', 'RBS', 'RBR', 'EX', 'VB', 'RB', 'FW', 'JJR', 'NNS', 'SYM', 'NNPS', 'LS', 'PRP$', 'VBN', 'PDT', 'UH', 'POS', 'WP$', 'TO', 'DT', 'VBP', 'JJS', 'WDT', 'NN', \"''\", 'PRP', 'JJ', 'NNP', 'VBD', ':', '.', ',', 'VBZ', '-RRB-'}\n",
      "45 45 45\n"
     ]
    }
   ],
   "source": [
    "train_folders = [f\"{i:02d}\" for i in range(0, 19)]   # Folders 00 to 18\n",
    "val_folders   = [f\"{i:02d}\" for i in range(19, 22)]    # Folders 19 to 21\n",
    "test_folders  = [f\"{i:02d}\" for i in range(22, 25)]    # Folders 22 to 24\n",
    "\n",
    "train_tokens, train_tags = load_data_from_folders(train_folders, base_path=\"wsj\")\n",
    "val_tokens, val_tags = load_data_from_folders(val_folders, base_path=\"wsj\")\n",
    "test_tokens, test_tags = load_data_from_folders(test_folders, base_path=\"wsj\")\n",
    "\n",
    "data_train = {\"tokens\": train_tokens, \"pos_tags\": train_tags}\n",
    "data_val   = {\"tokens\": val_tokens, \"pos_tags\": val_tags}\n",
    "data_test  = {\"tokens\": test_tokens, \"pos_tags\": test_tags}\n",
    "\n",
    "# i wanna see a ll the unique pos tags in the dataset\n",
    "# Flatten the list of lists to get all tags and then find unique tags\n",
    "unique_train_tags = set(tag for tags in train_tags for tag in tags)\n",
    "print(\"a \", unique_train_tags)\n",
    "\n",
    "unique_val_tags = set(tag for tags in val_tags for tag in tags)\n",
    "print(\"b \", unique_val_tags)\n",
    "\n",
    "unique_test_tags = set(tag for tags in test_tags for tag in tags)\n",
    "print(\"c \", unique_test_tags)\n",
    "\n",
    "print(len(unique_train_tags),len(unique_val_tags),len(unique_test_tags))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "label_list = sorted({tag for tags in data_train[\"pos_tags\"] for tag in tags})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "#use bert for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_list), label2id=label_to_id, id2label={i: label for i, label in enumerate(label_list)})\n",
    "#set the device to cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Align Labels üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454f97f820214034979e351906e556e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418689c9400145aa95b8ed1725d9c0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588ff2ff46a7479ea1b868444417908c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 38219\n",
      "Validation dataset size: 5527\n",
      "Test dataset size: 5462\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Assign the label for the first token of the word\n",
    "                label_ids.append(label_to_id.get(labels[word_idx], -100))\n",
    "            else:\n",
    "                # Assign -100 to subsequent tokens of the same word\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert datasets to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_dict(data_train)\n",
    "val_dataset = Dataset.from_dict(data_val)\n",
    "test_dataset = Dataset.from_dict(data_test)\n",
    "\n",
    "# Apply the tokenization and alignment function\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ü¶æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rinal\\Desktop\\UNIBO\\NLP2\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\rinal\\AppData\\Local\\Temp\\ipykernel_11512\\3122269356.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3585' max='3585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3585/3585 2:00:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.072144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.069548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.068169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./results\\\\tokenizer_config.json',\n",
       " './results\\\\special_tokens_map.json',\n",
       " './results\\\\vocab.txt',\n",
       " './results\\\\added_tokens.json',\n",
       " './results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Save the model\n",
    "trainer.save_model(\"./results\")\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation üë®‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 01:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:24<00:00,  4.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# Load the model for evaluation\n",
    "model = BertForTokenClassification.from_pretrained(\"./results\", num_labels=len(label_list), label2id=label_to_id, id2label={i: label for i, label in enumerate(label_list)})\n",
    "model.to(device)\n",
    "\n",
    "# Define the evaluation metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=data_collator)\n",
    "# Initialize lists to store predictions and labels\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Iterate over the test dataset\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Get the model predictions\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        # Get the predicted labels\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        # Append the predictions and labels to the lists\n",
    "        predictions_list.extend(predictions.cpu().numpy())\n",
    "        labels_list.extend(batch[\"labels\"].cpu().numpy())\n",
    "# Flatten the lists of predictions and labels\n",
    "predictions_flat = [item for sublist in predictions_list for item in sublist]\n",
    "labels_flat = [item for sublist in labels_list for item in sublist]\n",
    "# Remove ignored index (-100) from predictions and labels\n",
    "predictions_flat = [p for p, l in zip(predictions_flat, labels_flat) if l != -100]\n",
    "labels_flat = [l for l in labels_flat if l != -100]\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(np.array(predictions_flat) == np.array(labels_flat)) / len(labels_flat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9773\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           #       1.00      1.00      1.00        15\n",
      "           $       1.00      1.00      1.00       943\n",
      "          ''       1.00      1.00      1.00      1045\n",
      "           ,       1.00      1.00      1.00      6876\n",
      "       -LRB-       0.99      1.00      0.99       186\n",
      "       -RRB-       1.00      1.00      1.00       187\n",
      "           .       1.00      1.00      1.00      5381\n",
      "           :       1.00      1.00      1.00       752\n",
      "          CC       1.00      1.00      1.00      3250\n",
      "          CD       0.99      0.99      0.99      4823\n",
      "          DT       1.00      0.99      0.99     11183\n",
      "          EX       0.97      1.00      0.98       126\n",
      "          FW       0.43      0.30      0.35        30\n",
      "          IN       0.98      0.99      0.98     13492\n",
      "          JJ       0.94      0.94      0.94      8215\n",
      "         JJR       0.91      0.95      0.93       423\n",
      "         JJS       0.96      0.98      0.97       267\n",
      "          LS       1.00      0.73      0.85        15\n",
      "          MD       1.00      1.00      1.00      1267\n",
      "          NN       0.98      0.97      0.98     17834\n",
      "         NNP       0.99      0.96      0.97     13177\n",
      "        NNPS       0.27      0.78      0.40       170\n",
      "         NNS       0.99      0.99      0.99      8061\n",
      "         PDT       0.62      0.77      0.69        44\n",
      "         POS       0.99      0.99      0.99      1276\n",
      "         PRP       1.00      1.00      1.00      2205\n",
      "        PRP$       1.00      1.00      1.00      1068\n",
      "          RB       0.94      0.92      0.93      4405\n",
      "         RBR       0.86      0.82      0.84       271\n",
      "         RBS       0.93      0.90      0.91        69\n",
      "          RP       0.83      0.89      0.86       397\n",
      "         SYM       1.00      0.82      0.90        11\n",
      "          TO       1.00      1.00      1.00      2913\n",
      "          UH       0.76      0.76      0.76        17\n",
      "          VB       0.99      0.98      0.98      3573\n",
      "         VBD       0.99      0.97      0.98      4561\n",
      "         VBG       0.94      0.95      0.94      1933\n",
      "         VBN       0.91      0.94      0.92      2707\n",
      "         VBP       0.97      0.99      0.98      1565\n",
      "         VBZ       0.99      0.99      0.99      2639\n",
      "         WDT       0.98      0.98      0.98       584\n",
      "          WP       0.99      0.99      0.99       283\n",
      "         WP$       1.00      1.00      1.00        37\n",
      "         WRB       1.00      1.00      1.00       304\n",
      "          ``       1.00      1.00      1.00      1074\n",
      "\n",
      "    accuracy                           0.98    129654\n",
      "   macro avg       0.93      0.94      0.93    129654\n",
      "weighted avg       0.98      0.98      0.98    129654\n",
      "\n",
      "Balanced Accuracy: 0.9387\n",
      "F1 Score: 0.9780\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report,balanced_accuracy_score,f1_score,accuracy_score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels_flat, predictions_flat, target_names=label_list))\n",
    "\n",
    "#save in a text file\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(classification_report(labels_flat, predictions_flat, target_names=label_list))\n",
    "\n",
    "# Calculate and print balanced accuracy\n",
    "balanced_acc = balanced_accuracy_score(labels_flat, predictions_flat)\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "# Calculate and print F1 score\n",
    "f1 = f1_score(labels_flat, predictions_flat, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "# Calculate and print accuracy score\n",
    "acc_score = accuracy_score(labels_flat, predictions_flat)\n",
    "\n",
    "#save these metrics in a text file\n",
    "with open(\"metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Balanced Accuracy: {balanced_acc:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "    f.write(f\"Accuracy Score: {acc_score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "# Define a function to pick a random sentence from the test dataset and perform POS tagging\n",
    "def random_sentence_pos_tagging():\n",
    "    # Pick a random folder and file\n",
    "    random_folder = random.choice(test_folders)\n",
    "    folder_path = os.path.join(\"wsj\", random_folder)\n",
    "    random_file = random.choice(glob.glob(os.path.join(folder_path, \"*.conllx\")))\n",
    "    \n",
    "    # Read sentences from the selected file\n",
    "    sentences_tokens, _ = read_conllx(random_file)\n",
    "    \n",
    "    # Pick a random sentence from the file\n",
    "    random_index = random.randint(0, len(sentences_tokens) - 1)\n",
    "    sentence = \" \".join(sentences_tokens[random_index])\n",
    "    \n",
    "    # Tokenize the input sentence\n",
    "    words = sentence.split()\n",
    "    tokenized_input = tokenizer(words, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    tokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Map predictions to labels\n",
    "    predicted_labels = [model.config.id2label[label_id] for label_id in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    # Create a result dictionary\n",
    "    result = [{\"word\": word, \"label\": label} for word, label in zip(words, predicted_labels)]\n",
    "    \n",
    "    # Return the sentence, result, folder, and file\n",
    "    return f\"Folder: {random_folder}, File: {os.path.basename(random_file)}\", sentence, result\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=random_sentence_pos_tagging,\n",
    "    inputs=None,\n",
    "    outputs=[\"text\", \"text\", \"json\"],\n",
    "    title=\"Random Sentence POS Tagging\",\n",
    "    description=\"Click the button to pick a random sentence from folders 22 to 24 and get its POS tags.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
